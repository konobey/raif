{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Новые фичи\n",
    "Цифры по mcc  \n",
    "Погода по месту  \n",
    "расстояние до дальнейшего соседа  \n",
    "максимальная продолжительность приобретений в данной точке по дням"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Boosters] Raiffeisen Data Cup. Baseline\n",
    "Общий подход:\n",
    "- Добавляем к каждой транзакции столбец: is_work (если транзакция находится в пределах 0.02 от дома клиента)\n",
    "- Добавляем к каждой транзакции столбец: is_home (если транзакция находится в пределах 0.02 от работы клиента)\n",
    "- Обучаем классификатор предсказывающий вероятность (is_home == 1) для транзакции\n",
    "- Обучаем классификатор предсказывающий вероятность (is_work == 1) для транзакции\n",
    "\n",
    "Точность определения местоположения:\n",
    "- для классификатора is_home: ~3x%\n",
    "- для классификатора is_work: ~2x%\n",
    "- общая оценка на Public Leaderboard: ???\n",
    "\n",
    "Примечание\n",
    "* Требуется Python версии 3.5\n",
    "* Требуется библиотека xgboost (для обучения использовалась xgboost версии 0.7.post3)\n",
    "* Требуются файлы: test_set.csv, train_set.csv в одном каталоге с данным скриптом\n",
    "* Требования к памяти: должно работать с 2Гб свободного RAM\n",
    "* Время работы: ~3 минуты (тестировалось на процессоре Intel Core i7-4770)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['indices']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "MODULES_PATH = '../code/'\n",
    "if MODULES_PATH not in sys.path:\n",
    "    sys.path.append(MODULES_PATH)\n",
    "import mfuncs\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_colwidth = -1\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import gmaps\n",
    "API_KEY = 'AIzaSyCG_RL0_kavuEaJAqEN5xXbU4h0VJUbA9M'\n",
    "gmaps.configure(api_key=API_KEY) # Your Google API key\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим типы колонок для экономии памяти\n",
    "dtypes = {\n",
    "    'transaction_date': str,\n",
    "    'atm_address': str,\n",
    "    'country': str,\n",
    "    'city': str,\n",
    "    'amount': np.float32,\n",
    "    'currency': np.float32,\n",
    "    'mcc': str,\n",
    "    'customer_id': str,\n",
    "    'pos_address': str,\n",
    "    'atm_address': str,\n",
    "    'pos_lat': np.float32,\n",
    "    'pos_lon': np.float32,\n",
    "    'atm_lat': np.float32,\n",
    "    'atm_lon': np.float32,\n",
    "    'home_lat': np.float32,\n",
    "    'home_lon': np.float32,\n",
    "    'work_lat': np.float32,\n",
    "    'work_lon': np.float32,\n",
    "}\n",
    "df_all = pd.read_csv('../data/df_all.csv', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обрабатываем дату транзакции и категориальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['currency'] = df_all['currency'].fillna(-1).astype(np.int32)\n",
    "df_all['mcc'] = df_all['mcc'].apply(lambda x: int(x.replace(',', ''))).astype(np.int32)\n",
    "df_all['city'] = df_all['city_name'].factorize()[0].astype(np.int32)\n",
    "df_all['country'] = df_all['country'].factorize()[0].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фичи для даты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем транзакции без даты\n",
    "df_all = df_all[~df_all['transaction_date'].isnull()]\n",
    "df_all['transaction_date'] =  pd.to_datetime(df_all['transaction_date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['month'] = df_all.transaction_date.dt.month\n",
    "df_all['day'] = df_all.transaction_date.dt.day\n",
    "df_all['dayofyear'] = df_all.transaction_date.dt.dayofyear\n",
    "df_all['dayofweek'] = df_all.transaction_date.dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# праздники\n",
    "holidays_df = pd.read_csv('../data/internal/all_holidays.csv', header=None)\n",
    "holidays_df[0] = pd.to_datetime(holidays_df[0])\n",
    "holidays_df = holidays_df[holidays_df[0].dt.year == 2017]\n",
    "holidays = holidays_df[0].dt.dayofyear.values\n",
    "df_all['is_weekend'] = (df_all.dayofweek >= 6).astype(np.int8)\n",
    "df_all['is_state_holiday'] = df_all['dayofyear'].isin(holidays).astype(np.int8)\n",
    "df_all['is_holiday'] = df_all['is_weekend'] | df_all['is_state_holiday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Приводим адрес транзакции для pos и atm-транзакций к единообразному виду\n",
    "Просто объединяем в одну колонку и добавляем фичу - это атм или пос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['is_atm'] = (~df_all['atm_lat'].isnull()).astype(np.int8)\n",
    "df_all['is_pos'] = (~df_all['pos_lat'].isnull()).astype(np.int8)\n",
    "\n",
    "df_all['add_lat'] = df_all['atm_lat'].fillna(0) + df_all['pos_lat'].fillna(0)\n",
    "df_all['add_lon'] = df_all['atm_lon'].fillna(0) + df_all['pos_lon'].fillna(0)\n",
    "\n",
    "df_all.drop(['atm_lat','atm_lon','pos_lat','pos_lon'], axis=1, inplace=True)\n",
    "\n",
    "df_all = df_all[~((df_all['add_lon'] == 0) & (df_all['add_lon'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.5 s, sys: 75.6 ms, total: 4.58 s\n",
      "Wall time: 4.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# грязный хак, чтобы не учить КНН на новом юзере каждый раз\n",
    "df_all['fake_customer_id'] = (pd.factorize(df_all.customer_id)[0] + 1) * 100\n",
    "\n",
    "points = df_all[['fake_customer_id', 'add_lat', 'add_lon']].drop_duplicates().values\n",
    "neigh = NearestNeighbors(2, radius=100000)\n",
    "\n",
    "# расстояние до уникальных точек\n",
    "# neigh.fit(np.unique(points, axis=1))\n",
    "neigh.fit(points) \n",
    "\n",
    "distances, indices = neigh.kneighbors(df_all[['fake_customer_id', 'add_lat', 'add_lon']].values)\n",
    "df_all['distance_to_nearest_point'] = distances[:, 1]\n",
    "del df_all['fake_customer_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>add_lat</th>\n",
       "      <th>add_lon</th>\n",
       "      <th>cl_label</th>\n",
       "      <th>cl_score</th>\n",
       "      <th>cl_lat</th>\n",
       "      <th>cl_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0dc0137d280a2a82d2dc89282450ff1b</td>\n",
       "      <td>59.844074</td>\n",
       "      <td>30.179153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933796</td>\n",
       "      <td>59.841676</td>\n",
       "      <td>30.177239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0dc0137d280a2a82d2dc89282450ff1b</td>\n",
       "      <td>59.844074</td>\n",
       "      <td>30.179153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933796</td>\n",
       "      <td>59.841676</td>\n",
       "      <td>30.177239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0dc0137d280a2a82d2dc89282450ff1b</td>\n",
       "      <td>59.858200</td>\n",
       "      <td>30.229023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.551394</td>\n",
       "      <td>59.865354</td>\n",
       "      <td>30.247539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0dc0137d280a2a82d2dc89282450ff1b</td>\n",
       "      <td>59.844074</td>\n",
       "      <td>30.179153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933796</td>\n",
       "      <td>59.841676</td>\n",
       "      <td>30.177239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0dc0137d280a2a82d2dc89282450ff1b</td>\n",
       "      <td>59.844074</td>\n",
       "      <td>30.179153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933796</td>\n",
       "      <td>59.841676</td>\n",
       "      <td>30.177239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        customer_id    add_lat    add_lon  cl_label  cl_score  \\\n",
       "0  0dc0137d280a2a82d2dc89282450ff1b  59.844074  30.179153  0.0       0.933796   \n",
       "1  0dc0137d280a2a82d2dc89282450ff1b  59.844074  30.179153  0.0       0.933796   \n",
       "2  0dc0137d280a2a82d2dc89282450ff1b  59.858200  30.229023  1.0       0.551394   \n",
       "3  0dc0137d280a2a82d2dc89282450ff1b  59.844074  30.179153  0.0       0.933796   \n",
       "4  0dc0137d280a2a82d2dc89282450ff1b  59.844074  30.179153  0.0       0.933796   \n",
       "\n",
       "      cl_lat     cl_lon  \n",
       "0  59.841676  30.177239  \n",
       "1  59.841676  30.177239  \n",
       "2  59.865354  30.247539  \n",
       "3  59.841676  30.177239  \n",
       "4  59.841676  30.177239  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# кластерные фичи\n",
    "df_cluster = pd.read_csv('../data/df_cluster.csv')\n",
    "df_cluster.reset_index(drop=True, inplace=True)\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "df_all = pd.concat([df_all, df_cluster.iloc[:, 3:]], axis=1)\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем признаки is_home, is_work\n",
    "TODO: удалить чуваков у которых несколько домов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = df_all['home_lat'] - df_all['add_lat']\n",
    "lon = df_all['home_lon'] - df_all['add_lon']\n",
    "\n",
    "df_all['is_home'] = (np.sqrt((lat ** 2) + (lon ** 2)) <= 0.02).astype(np.int8)\n",
    "df_all['has_home'] = (~df_all['home_lon'].isnull()).astype(np.int8)\n",
    "\n",
    "lat = df_all['work_lat'] - df_all['add_lat']\n",
    "lon = df_all['work_lon'] - df_all['add_lon']\n",
    "df_all['is_work'] = (np.sqrt((lat ** 2) + (lon ** 2)) <= 0.02).astype(np.int8)\n",
    "df_all['has_work'] = (~df_all['work_lon'].isnull()).astype(np.int8)\n",
    "\n",
    "# df_all.drop(['work_lat','work_lon','home_lat','home_lon'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем категориальный признак для адреса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['address'] = df_all['add_lat'].apply(lambda x: \"%.02f\" % x) + ';' + df_all['add_lon'].apply(lambda x: \"%.02f\" % x)\n",
    "df_all['address'] = df_all['address'].factorize()[0].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем абонентские фичи отвечающие за соотношения между точками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.merge(df_all.groupby('customer_id')['amount'].count().reset_index(name='cid_trans_count'), how='left')\n",
    "df_all['cid_trans_count'] = df_all['cid_trans_count'].astype(np.int32)\n",
    "\n",
    "df_all = df_all.merge(df_all.groupby('customer_id')['amount'].agg('sum').reset_index(name='cid_trans_sum'), how='left')\n",
    "df_all['cid_trans_sum'] = df_all['cid_trans_sum'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_count_sum_ratios(df_all, col):\n",
    "    col_count = 'cid_{}_trans_count'.format(col)\n",
    "    col_sum = 'cid_{}_trans_sum'.format(col)\n",
    "    df_ = df_all.groupby(['customer_id', col])['amount'].count().reset_index(name=col_count)\n",
    "    df_all = df_all.merge(df_, how='left')\n",
    "    df_all[col_count] = df_all[col_count].astype(np.int32)\n",
    "    df_all['ratio_{}_count'.format(col)] = df_all[col_count] / df_all['cid_trans_count']\n",
    "    \n",
    "    df_ = df_all.groupby(['customer_id', col])['amount'].agg('sum').reset_index(name=col_sum)\n",
    "    df_all = df_all.merge(df_, how='left')\n",
    "    df_all[col_sum] = df_all[col_sum].astype(np.float32)\n",
    "    df_all['ratio_{}_sum'.format(col)] = df_all[col_sum] / df_all['cid_trans_sum']\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = add_count_sum_ratios(df_all, 'address')\n",
    "df_all = add_count_sum_ratios(df_all, 'terminal_id')\n",
    "df_all = add_count_sum_ratios(df_all, 'mcc')\n",
    "df_all = add_count_sum_ratios(df_all, 'is_holiday')\n",
    "df_all = add_count_sum_ratios(df_all, 'city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мои фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# добавим признаки после групбая\n",
    "df_gb = df_all[['customer_id','amount', 'add_lat', 'add_lon']].groupby('customer_id')\n",
    "coord_stat_df = df_gb.agg(['mean', 'max', 'min'])\n",
    "coord_stat_df['transactions_per_user'] = df_gb.agg('size')\n",
    "coord_stat_df.columns = ['_'.join(col).strip() for col in coord_stat_df.columns.values]\n",
    "coord_stat_df.reset_index(inplace=True)\n",
    "df_all = pd.merge(df_all, coord_stat_df, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['add_lat', 'add_lon']\n",
    "types = ['min', 'max', 'mean']\n",
    "for c in cols:\n",
    "    for t in types:\n",
    "        df_all['{}_diff_{}'.format(c, t)] = np.abs(df_all[c] - df_all['{}_{}'.format(c, t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разности \n",
    "df_all['lat_diff_cluster_lat'] = np.abs(df_all['add_lat'] - df_all['cl_lat'])\n",
    "df_all['lon_diff_cluster_lon'] = np.abs(df_all['add_lon'] - df_all['cl_lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фичи mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcc_str</th>\n",
       "      <th>mcc_cat1</th>\n",
       "      <th>mcc_cat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0005</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mcc_str  mcc_cat1  mcc_cat2\n",
       "1  0001   -1         0       \n",
       "2  0002   -1         0       \n",
       "3  0003   -1         0       \n",
       "4  0004   -1         0       \n",
       "5  0005   -1         0       "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# категории\n",
    "df_all['mcc_str'] = df_all['mcc'].astype(str).str.rjust(4, '0')\n",
    "df_mcc = pd.read_csv('../data/internal/mcc.csv')\n",
    "df_mcc = df_mcc.iloc[1:, :3]\n",
    "df_mcc.columns = ['mcc_str', 'mcc_cat1', 'mcc_cat2']\n",
    "df_mcc.drop_duplicates(subset=['mcc_str'], inplace=True)\n",
    "df_mcc['mcc_cat1'] = pd.factorize(df_mcc['mcc_cat1'])[0]\n",
    "df_mcc['mcc_cat2'] = pd.factorize(df_mcc['mcc_cat2'])[0]\n",
    "df_mcc.fillna('none', inplace=True)\n",
    "df_all = pd.merge(df_all, df_mcc, on='mcc_str', how='left')\n",
    "del df_all['mcc_str']\n",
    "df_mcc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcc['mcc_cat1'].fillna(-1, inplace=True)\n",
    "df_mcc['mcc_cat2'].fillna(-1, inplace=True)\n",
    "\n",
    "df_all = add_count_sum_ratios(df_all, 'mcc_cat1')\n",
    "df_all = add_count_sum_ratios(df_all, 'mcc_cat2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# частота mcc\n",
    "df_mcc = df_all['mcc'].value_counts(normalize=True).reset_index()\n",
    "df_mcc.columns = ['mcc', 'mcc_freq']\n",
    "df_all = pd.merge(df_all, df_mcc, on='mcc', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all, pd.get_dummies(df_all['mcc'], prefix='mcc')], axis=1)\n",
    "del df_all['mcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all, pd.get_dummies(df_all['mcc_cat1'], prefix='mcc_cat1')], axis=1)\n",
    "del df_all['mcc_cat1']\n",
    "\n",
    "df_all = pd.concat([df_all, pd.get_dummies(df_all['mcc_cat2'], prefix='mcc_cat2')], axis=1)\n",
    "del df_all['mcc_cat2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем групбай какие вообще есть mcc у посетителя. Это поможет понять его привычки\n",
    "mcc_cols = [c for c in df_all.columns if 'mcc' in c and 'cat' not in c]\n",
    "df_mcc = df_all.groupby('customer_id')[mcc_cols].agg(['max', 'mean'])\n",
    "df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "df_mcc.reset_index(inplace=True)\n",
    "df_mcc.head()\n",
    "df_all = pd.merge(df_all, df_mcc, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем групбай какие вообще есть mcc у посетителя. Это поможет понять его привычки\n",
    "mcc_cols = [c for c in df_all.columns if 'mcc_cat1' in c]\n",
    "df_mcc = df_all.groupby('customer_id')[mcc_cols].agg(['max', 'mean'])\n",
    "df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "df_mcc.reset_index(inplace=True)\n",
    "df_mcc.head()\n",
    "df_all = pd.merge(df_all, df_mcc, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем групбай какие вообще есть mcc у посетителя. Это поможет понять его привычки\n",
    "mcc_cols = [c for c in df_all.columns if 'mcc_cat2' in c]\n",
    "df_mcc = df_all.groupby('customer_id')[mcc_cols].agg(['max', 'mean'])\n",
    "df_mcc.columns = ['_'.join(col).strip() for col in df_mcc.columns.values]\n",
    "df_mcc.reset_index(inplace=True)\n",
    "df_mcc.head()\n",
    "df_all = pd.merge(df_all, df_mcc, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Игрушки с адресами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['string'].fillna('', inplace=True)\n",
    "df_all['string'] = df_all['string'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all['has_street'] = df_all['string'].str.contains('улиц').fillna(0).astype(np.int8)\n",
    "df_all['has_pereul'] = df_all['string'].str.contains('переул').fillna(0).astype(np.int8)\n",
    "df_all['has_bulvar'] = df_all['string'].str.contains('бульв').fillna(0).astype(np.int8)\n",
    "df_all['has_prospekt'] = df_all['string'].str.contains('проспект').fillna(0).astype(np.int8)\n",
    "df_all['has_shosse'] = df_all['string'].str.contains('шосс').fillna(0).astype(np.int8)\n",
    "\n",
    "df_all['has_torg'] = df_all['string'].str.contains('торгов').astype(np.int8)\n",
    "df_all['has_bus'] = df_all['string'].str.contains('бизн').astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2294265, 1703), 0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape, df_all.columns.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.loc[:,~df_all.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ys = ['is_home', 'is_work']\n",
    "drop_cols = ['atm_address', 'customer_id', 'pos_address', 'terminal_id', 'transaction_date', \n",
    "             'is_home' ,'has_home', 'is_work', 'has_work', 'is_train', 'city_name']\n",
    "drop_cols += ['work_lat','work_lon','home_lat','home_lon', 'string']\n",
    "\n",
    "drop_cols += ['pred:is_home', 'pred:is_work']\n",
    "y_cols = ['is_home', 'is_work']\n",
    "usecols = df_all.drop(drop_cols, 1, errors='ignore').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 63,\n",
    "    'learning_rate': 0.01,\n",
    "    'metric' : 'binary_logloss',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'num_threads': 12,\n",
    "    'verbose': 0,\n",
    "}\n",
    "\n",
    "model = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds.\n",
      "[30]\tvalid_0's binary_logloss: 0.596722\n",
      "[60]\tvalid_0's binary_logloss: 0.538552\n",
      "[90]\tvalid_0's binary_logloss: 0.501204\n",
      "[120]\tvalid_0's binary_logloss: 0.476618\n",
      "[150]\tvalid_0's binary_logloss: 0.459465\n",
      "[180]\tvalid_0's binary_logloss: 0.447449\n",
      "[210]\tvalid_0's binary_logloss: 0.438473\n",
      "[240]\tvalid_0's binary_logloss: 0.432042\n",
      "[270]\tvalid_0's binary_logloss: 0.42723\n",
      "[300]\tvalid_0's binary_logloss: 0.423608\n",
      "[330]\tvalid_0's binary_logloss: 0.420977\n",
      "[360]\tvalid_0's binary_logloss: 0.419002\n",
      "[390]\tvalid_0's binary_logloss: 0.417224\n",
      "[420]\tvalid_0's binary_logloss: 0.415874\n",
      "[450]\tvalid_0's binary_logloss: 0.41505\n",
      "[480]\tvalid_0's binary_logloss: 0.414468\n",
      "[510]\tvalid_0's binary_logloss: 0.41379\n",
      "[540]\tvalid_0's binary_logloss: 0.41319\n",
      "[570]\tvalid_0's binary_logloss: 0.412762\n",
      "[600]\tvalid_0's binary_logloss: 0.412645\n",
      "[630]\tvalid_0's binary_logloss: 0.412296\n",
      "[660]\tvalid_0's binary_logloss: 0.41221\n",
      "[690]\tvalid_0's binary_logloss: 0.41179\n",
      "[720]\tvalid_0's binary_logloss: 0.411611\n",
      "[750]\tvalid_0's binary_logloss: 0.411527\n",
      "[780]\tvalid_0's binary_logloss: 0.411512\n",
      "[810]\tvalid_0's binary_logloss: 0.411305\n",
      "[840]\tvalid_0's binary_logloss: 0.411347\n",
      "[870]\tvalid_0's binary_logloss: 0.411282\n",
      "[900]\tvalid_0's binary_logloss: 0.411238\n",
      "[930]\tvalid_0's binary_logloss: 0.411136\n",
      "[960]\tvalid_0's binary_logloss: 0.411355\n",
      "[990]\tvalid_0's binary_logloss: 0.411415\n",
      "[1020]\tvalid_0's binary_logloss: 0.411299\n",
      "[1050]\tvalid_0's binary_logloss: 0.411162\n",
      "[1080]\tvalid_0's binary_logloss: 0.411278\n",
      "[1110]\tvalid_0's binary_logloss: 0.411221\n",
      "[1140]\tvalid_0's binary_logloss: 0.411294\n",
      "[1170]\tvalid_0's binary_logloss: 0.411138\n",
      "[1200]\tvalid_0's binary_logloss: 0.411324\n",
      "Early stopping, best iteration is:\n",
      "[927]\tvalid_0's binary_logloss: 0.411109\n"
     ]
    }
   ],
   "source": [
    "y_col = 'is_home'\n",
    "\n",
    "cust_train = df_all[df_all['is_train']==1].groupby('customer_id')[y_col.replace('is_','has_')].max()\n",
    "cust_train = cust_train[cust_train > 0].index\n",
    "\n",
    "cust_train, cust_valid = train_test_split(cust_train, test_size=0.2, shuffle=True, random_state=111)\n",
    "\n",
    "df_train = pd.DataFrame(cust_train, columns=['customer_id']).merge(df_all, how='left')\n",
    "df_valid = pd.DataFrame(cust_valid, columns=['customer_id']).merge(df_all, how='left')\n",
    "\n",
    "lgb_train = lgb.Dataset(df_train[usecols], df_train[y_col])\n",
    "lgb_valid = lgb.Dataset(df_valid[usecols], df_valid[y_col])\n",
    "\n",
    "gbm_h = lgb.train(params,\n",
    "                lgb_train,\n",
    "                valid_sets=[lgb_valid],\n",
    "                num_boost_round=2000,\n",
    "                verbose_eval=30,\n",
    "                early_stopping_rounds=300)\n",
    "\n",
    "model[y_col] = gbm_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds.\n",
      "[30]\tvalid_0's binary_logloss: 0.576869\n",
      "[60]\tvalid_0's binary_logloss: 0.507223\n",
      "[90]\tvalid_0's binary_logloss: 0.462538\n",
      "[120]\tvalid_0's binary_logloss: 0.432042\n",
      "[150]\tvalid_0's binary_logloss: 0.410963\n",
      "[180]\tvalid_0's binary_logloss: 0.396125\n",
      "[210]\tvalid_0's binary_logloss: 0.385221\n",
      "[240]\tvalid_0's binary_logloss: 0.377783\n",
      "[270]\tvalid_0's binary_logloss: 0.372313\n",
      "[300]\tvalid_0's binary_logloss: 0.36795\n",
      "[330]\tvalid_0's binary_logloss: 0.364977\n",
      "[360]\tvalid_0's binary_logloss: 0.362587\n",
      "[390]\tvalid_0's binary_logloss: 0.360699\n",
      "[420]\tvalid_0's binary_logloss: 0.359398\n",
      "[450]\tvalid_0's binary_logloss: 0.358296\n",
      "[480]\tvalid_0's binary_logloss: 0.3579\n",
      "[510]\tvalid_0's binary_logloss: 0.3574\n",
      "[540]\tvalid_0's binary_logloss: 0.356931\n",
      "[570]\tvalid_0's binary_logloss: 0.356533\n",
      "[600]\tvalid_0's binary_logloss: 0.356384\n",
      "[630]\tvalid_0's binary_logloss: 0.356606\n",
      "[660]\tvalid_0's binary_logloss: 0.356522\n",
      "[690]\tvalid_0's binary_logloss: 0.356314\n",
      "[720]\tvalid_0's binary_logloss: 0.356535\n",
      "[750]\tvalid_0's binary_logloss: 0.35662\n",
      "[780]\tvalid_0's binary_logloss: 0.356801\n",
      "[810]\tvalid_0's binary_logloss: 0.356836\n",
      "[840]\tvalid_0's binary_logloss: 0.356812\n",
      "[870]\tvalid_0's binary_logloss: 0.357208\n",
      "[900]\tvalid_0's binary_logloss: 0.357381\n",
      "Early stopping, best iteration is:\n",
      "[613]\tvalid_0's binary_logloss: 0.356253\n"
     ]
    }
   ],
   "source": [
    "y_col = 'is_work'\n",
    "\n",
    "cust_train = df_all[df_all['is_train']==1].groupby('customer_id')[y_col.replace('is_','has_')].max()\n",
    "cust_train = cust_train[cust_train > 0].index\n",
    "\n",
    "cust_train, cust_valid = train_test_split(cust_train, test_size=0.2, shuffle=True, random_state=111)\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(cust_train, columns=['customer_id']).merge(df_all, how='left')\n",
    "df_valid = pd.DataFrame(cust_valid, columns=['customer_id']).merge(df_all, how='left')\n",
    "\n",
    "lgb_train = lgb.Dataset(df_train[usecols], df_train[y_col])\n",
    "lgb_valid = lgb.Dataset(df_valid[usecols], df_valid[y_col])\n",
    "\n",
    "gbm_w = lgb.train(params,\n",
    "                lgb_train,\n",
    "                valid_sets=[lgb_valid],\n",
    "                num_boost_round=2000,\n",
    "                verbose_eval=30,\n",
    "                early_stopping_rounds=300)\n",
    "\n",
    "model[y_col] = gbm_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(gbm_h, max_num_features=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _best(x):\n",
    "    ret = None\n",
    "    for col in ys:\n",
    "        pred = ('pred:%s' % col)\n",
    "        if pred in x:\n",
    "            i = (x[pred].idxmax())\n",
    "            cols = [pred, 'add_lat', 'add_lon']\n",
    "            if col in x:\n",
    "                cols.append(col)\n",
    "            tmp = x.loc[i,cols]\n",
    "            tmp.rename({\n",
    "                'add_lat':'%s:add_lat' % col,\n",
    "                'add_lon':'%s:add_lon' % col,\n",
    "            }, inplace = True)\n",
    "            if ret is None:\n",
    "                ret = tmp\n",
    "            else:\n",
    "                ret = pd.concat([ret, tmp])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def predict_proba(dt, ys=['is_home', 'is_work']):\n",
    "    for col in ys:\n",
    "        pred = ('pred:%s' % col)\n",
    "        dt[pred] = model[col].predict(dt[usecols])\n",
    "    return dt.groupby('customer_id').apply(_best).reset_index()\n",
    "\n",
    "def score(dt, ys=['is_home', 'is_work'], return_df=False):\n",
    "    dt_ret = predict_proba(dt, ys)\n",
    "    if return_df:\n",
    "        return dt_ret\n",
    "    mean = 0.0\n",
    "    for col in ys:\n",
    "        col_mean = dt_ret[col].mean()\n",
    "        mean += col_mean\n",
    "    if len(ys) == 2:\n",
    "        mean = mean / len(ys)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5203587009209889\n",
      "Test accuracy: 0.5232558139534884\n",
      "Train accuracy: 0.3688802714493456\n",
      "Test accuracy: 0.34205426356589147\n"
     ]
    }
   ],
   "source": [
    "print (\"Train accuracy:\", score(df_train, ys=['is_home']))\n",
    "print (\"Test accuracy:\", score(df_valid, ys=['is_home']))\n",
    "\n",
    "print (\"Train accuracy:\", score(df_train, ys=['is_work']))\n",
    "print (\"Test accuracy:\", score(df_valid, ys=['is_work']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "до ohe\n",
    "Train accuracy: 0.5087251575375666\n",
    "Test accuracy: 0.5077519379844961\n",
    "Train accuracy: 0.3637905962190984\n",
    "Test accuracy: 0.33527131782945735\n",
    "после ohe\n",
    "Train accuracy: 0.5070285991274842\n",
    "Test accuracy: 0.5077519379844961\n",
    "Train accuracy: 0.3703344643722734\n",
    "Test accuracy: 0.3391472868217054\n",
    "после статистиик по всем mcc категориям\n",
    "Train accuracy: 0.5203587009209889\n",
    "Test accuracy: 0.5232558139534884\n",
    "Train accuracy: 0.3688802714493456\n",
    "Test accuracy: 0.34205426356589147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ False-Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# сколько вообще людей имеют хорошую точку\n",
    "df_all[(df_all.is_train == 1)].groupby('customer_id')['is_work'].agg('max').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = score(df_valid, ys=['is_home'], return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid = 'bf66305d0ec05abb6e6a6358acb8c2a1'\n",
    "cid = df_pred[df_pred.is_home == 0].sample(1)['customer_id'].values[0]\n",
    "\n",
    "df_an = df_all[df_all.customer_id == cid]\n",
    "center_home = df_an[['home_lat', 'home_lon']].drop_duplicates().values\n",
    "center_work = df_an[['work_lat', 'work_lon']].drop_duplicates().values\n",
    "\n",
    "\n",
    "predicted_home = df_pred[df_pred.customer_id == cid][['is_home:add_lat', 'is_home:add_lon']].drop_duplicates().values\n",
    "predicted_work = df_pred[df_pred.customer_id == cid][['is_work:add_lat', 'is_work:add_lon']].drop_duplicates().values\n",
    "\n",
    "points_pos = df_an[df_an.is_pos == 1][['add_lat', 'add_lon']].dropna().values\n",
    "points_atm = df_an[df_an.is_pos == 0][['add_lat', 'add_lon']].dropna().values\n",
    "print(center_home.shape, center_work.shape, points_pos.shape, points_atm.shape)\n",
    "\n",
    "# синие - покупки\n",
    "# красные - банкоматы\n",
    "gmap = gmaps.Map()\n",
    "if len(points_pos) > 0:\n",
    "    gmap.add_layer(gmaps.symbol_layer(points_pos, hover_text='pos', \n",
    "                                      fill_color=\"blue\", stroke_color=\"blue\", scale=3))\n",
    "if len(points_atm) > 0:\n",
    "    gmap.add_layer(gmaps.symbol_layer(points_atm, hover_text='atm',\n",
    "                                      fill_color=\"red\", stroke_color=\"red\",scale=3))\n",
    "\n",
    "if not np.isnan(center_home)[0][0]:\n",
    "    gmap.add_layer(gmaps.marker_layer(center_home, label='home'))\n",
    "if not np.isnan(center_work)[0][0]:\n",
    "    gmap.add_layer(gmaps.marker_layer(center_work, label='work'))\n",
    "\n",
    "gmap.add_layer(gmaps.marker_layer(predicted_home, label='predicted_home'))\n",
    "gmap.add_layer(gmaps.marker_layer(predicted_work, label='predicted_work'))\n",
    "    \n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_an"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_ID_</th>\n",
       "      <th>_WORK_LAT_</th>\n",
       "      <th>_WORK_LON_</th>\n",
       "      <th>_HOME_LAT_</th>\n",
       "      <th>_HOME_LON_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021683ccb416637fe9a4cd35e4606e</td>\n",
       "      <td>55.023354</td>\n",
       "      <td>82.914726</td>\n",
       "      <td>55.041771</td>\n",
       "      <td>82.984329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002d0f8a642272b41c292c12ab6e602</td>\n",
       "      <td>44.033001</td>\n",
       "      <td>42.835999</td>\n",
       "      <td>44.032093</td>\n",
       "      <td>42.837608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004d182d9fede3ba2534b2d5e5ad27e</td>\n",
       "      <td>43.585999</td>\n",
       "      <td>39.723999</td>\n",
       "      <td>43.572430</td>\n",
       "      <td>39.736073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008c2445518c9392cb356c5c3db3392</td>\n",
       "      <td>51.528755</td>\n",
       "      <td>46.040150</td>\n",
       "      <td>51.537647</td>\n",
       "      <td>46.017811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000b373cc4969c0be8e0933c08da67e1</td>\n",
       "      <td>56.237175</td>\n",
       "      <td>43.463005</td>\n",
       "      <td>56.232037</td>\n",
       "      <td>43.458107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               _ID_  _WORK_LAT_  _WORK_LON_  _HOME_LAT_  \\\n",
       "0  00021683ccb416637fe9a4cd35e4606e  55.023354   82.914726   55.041771    \n",
       "1  0002d0f8a642272b41c292c12ab6e602  44.033001   42.835999   44.032093    \n",
       "2  0004d182d9fede3ba2534b2d5e5ad27e  43.585999   39.723999   43.572430    \n",
       "3  0008c2445518c9392cb356c5c3db3392  51.528755   46.040150   51.537647    \n",
       "4  000b373cc4969c0be8e0933c08da67e1  56.237175   43.463005   56.232037    \n",
       "\n",
       "   _HOME_LON_  \n",
       "0  82.984329   \n",
       "1  42.837608   \n",
       "2  39.736073   \n",
       "3  46.017811   \n",
       "4  43.458107   "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_test = df_all[df_all['is_train'] == 0]['customer_id'].unique()\n",
    "df_test = pd.DataFrame(cust_test, columns = ['customer_id']).merge(df_all, how = 'left')\n",
    "df_test = predict_proba(df_test)\n",
    "df_test.rename(columns = {\n",
    "        'customer_id':'_ID_',\n",
    "        'is_home:add_lat': '_HOME_LAT_',\n",
    "        'is_home:add_lon': '_HOME_LON_',\n",
    "        'is_work:add_lat': '_WORK_LAT_',\n",
    "        'is_work:add_lon': '_WORK_LON_'}, inplace = True)\n",
    "df_test = df_test[['_ID_', '_WORK_LAT_', '_WORK_LON_', '_HOME_LAT_', '_HOME_LON_']]\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем submission-файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем пропуски\n",
    "df_ = pd.read_csv('../data/test_set.csv', dtype=dtypes, usecols=['customer_id'])\n",
    "submission = pd.DataFrame(df_['customer_id'].unique(), columns=['_ID_'])\n",
    "\n",
    "submission = submission.merge(df_test, how='left').fillna(0)\n",
    "# Пишем файл submission\n",
    "submission.to_csv('../submissions/base_3_523_342.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
