{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Boosters] Raiffeisen Data Cup. Baseline\n",
    "Общий подход:\n",
    "- Добавляем к каждой транзакции столбец: is_work (если транзакция находится в пределах 0.02 от дома клиента)\n",
    "- Добавляем к каждой транзакции столбец: is_home (если транзакция находится в пределах 0.02 от работы клиента)\n",
    "- Обучаем классификатор предсказывающий вероятность (is_home == 1) для транзакции\n",
    "- Обучаем классификатор предсказывающий вероятность (is_work == 1) для транзакции\n",
    "\n",
    "Точность определения местоположения:\n",
    "- для классификатора is_home: ~3x%\n",
    "- для классификатора is_work: ~2x%\n",
    "- общая оценка на Public Leaderboard: ???\n",
    "\n",
    "Примечание\n",
    "* Требуется Python версии 3.5\n",
    "* Требуется библиотека xgboost (для обучения использовалась xgboost версии 0.7.post3)\n",
    "* Требуются файлы: test_set.csv, train_set.csv в одном каталоге с данным скриптом\n",
    "* Требования к памяти: должно работать с 2Гб свободного RAM\n",
    "* Время работы: ~3 минуты (тестировалось на процессоре Intel Core i7-4770)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "MODULES_PATH = '../code/'\n",
    "if MODULES_PATH not in sys.path:\n",
    "    sys.path.append(MODULES_PATH)\n",
    "import mfuncs\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_columns = 1000\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим типы колонок для экономии памяти\n",
    "dtypes = {\n",
    "    'transaction_date': str,\n",
    "    'atm_address': str,\n",
    "    'country': str,\n",
    "    'city': str,\n",
    "    'amount': np.float32,\n",
    "    'currency': np.float32,\n",
    "    'mcc': str,\n",
    "    'customer_id': str,\n",
    "    'pos_address': str,\n",
    "    'atm_address': str,\n",
    "    'pos_adress_lat': np.float32,\n",
    "    'pos_adress_lon': np.float32,\n",
    "    'pos_address_lat': np.float32,\n",
    "    'pos_address_lon': np.float32,\n",
    "    'atm_address_lat': np.float32,\n",
    "    'atm_address_lon': np.float32,\n",
    "    'home_add_lat': np.float32,\n",
    "    'home_add_lon': np.float32,\n",
    "    'work_add_lat': np.float32,\n",
    "    'work_add_lon': np.float32,\n",
    "}\n",
    "\n",
    "# для экономии памяти будем загружать только часть атрибутов транзакций\n",
    "usecols_train = ['customer_id','transaction_date','amount','country', 'city', 'currency', 'mcc', 'pos_adress_lat', 'pos_adress_lon', 'atm_address_lat', 'atm_address_lon','home_add_lat','home_add_lon','work_add_lat','work_add_lon']\n",
    "usecols_test = ['customer_id','transaction_date','amount','country', 'city', 'currency', 'mcc', 'pos_address_lat', 'pos_address_lon', 'atm_address_lat', 'atm_address_lon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Читаем train_set, test_set, соединяем в один датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'transaction_date': str,\n",
    "    'atm_address': str,\n",
    "    'country': str,\n",
    "    'city': str,\n",
    "    'amount': np.float32,\n",
    "    'currency': np.float32,\n",
    "    'mcc': str,\n",
    "    'customer_id': str,\n",
    "    'pos_address': str,\n",
    "    'atm_address': str,\n",
    "    'pos_adress_lat': np.float32,\n",
    "    'pos_adress_lon': np.float32,\n",
    "    'pos_address_lat': np.float32,\n",
    "    'pos_address_lon': np.float32,\n",
    "    'atm_address_lat': np.float32,\n",
    "    'atm_address_lon': np.float32,\n",
    "    'home_add_lat': np.float32,\n",
    "    'home_add_lon': np.float32,\n",
    "    'work_add_lat': np.float32,\n",
    "    'work_add_lon': np.float32,\n",
    "}\n",
    "\n",
    "rnm = {\n",
    "    'atm_address_lat': 'atm_lat',\n",
    "    'atm_address_lon': 'atm_lon',\n",
    "    'pos_adress_lat': 'pos_lat',\n",
    "    'pos_adress_lon': 'pos_lon',\n",
    "    'pos_address_lat': 'pos_lat',\n",
    "    'pos_address_lon': 'pos_lon',\n",
    "    'home_add_lat': 'home_lat',\n",
    "    'home_add_lon': 'home_lon',\n",
    "    'work_add_lat': 'work_lat',\n",
    "    'work_add_lon': 'work_lon',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train_set.csv', dtype=dtypes)\n",
    "df_test = pd.read_csv('../data/test_set.csv', dtype=dtypes)\n",
    "\n",
    "df_train.rename(columns=rnm, inplace=True)\n",
    "df_test.rename(columns=rnm, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалим чувак с множественными адресами\n",
    "# print(df_train.shape)\n",
    "# gb = df_train.groupby('customer_id')['work_lat'].agg('nunique') \n",
    "# cid_incorrect = gb[gb == 2].index\n",
    "# df_train = df_train[~df_train.customer_id.isin(cid_incorrect.values)]\n",
    "# print(df_train.shape)\n",
    "# gb = df_train.groupby('customer_id')['home_lat'].agg('nunique') \n",
    "# cid_incorrect = gb[gb == 2].index\n",
    "# df_train = df_train[~df_train.customer_id.isin(cid_incorrect.values)]\n",
    "# print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# соединяем test/train в одном DataFrame\n",
    "df_train['is_train'] = np.int32(1)\n",
    "df_test['is_train'] = np.int32(0)\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "\n",
    "del df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обрабатываем дату транзакции и категориальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['currency'] = df_all['currency'].fillna(-1).astype(np.int32)\n",
    "df_all['mcc'] = df_all['mcc'].apply(lambda x: int(x.replace(',', ''))).astype(np.int32)\n",
    "df_all['city'] = df_all['city'].factorize()[0].astype(np.int32)\n",
    "df_all['country'] = df_all['country'].factorize()[0].astype(np.int32)\n",
    "\n",
    "# удаляем транзакции без даты\n",
    "df_all = df_all[~df_all['transaction_date'].isnull()]\n",
    "df_all['transaction_date'] =  pd.to_datetime(df_all['transaction_date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фичи для даты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['month'] = df_all.transaction_date.dt.month\n",
    "df_all['day'] = df_all.transaction_date.dt.day\n",
    "df_all['dayofyear'] = df_all.transaction_date.dt.dayofyear\n",
    "df_all['dayofweek'] = df_all.transaction_date.dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Приводим адрес транзакции для pos и atm-транзакций к единообразному виду\n",
    "Просто объединяем в одну колонку и добавляем фичу - это атм или пос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['is_atm'] = (~df_all['atm_lat'].isnull()).astype(np.int8)\n",
    "df_all['is_pos'] = (~df_all['pos_lat'].isnull()).astype(np.int8)\n",
    "\n",
    "df_all['add_lat'] = df_all['atm_lat'].fillna(0) + df_all['pos_lat'].fillna(0)\n",
    "df_all['add_lon'] = df_all['atm_lon'].fillna(0) + df_all['pos_lon'].fillna(0)\n",
    "\n",
    "df_all.drop(['atm_lat','atm_lon','pos_lat','pos_lon'], axis=1, inplace=True)\n",
    "\n",
    "df_all = df_all[~((df_all['add_lon'] == 0) & (df_all['add_lon'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.82 s, sys: 107 ms, total: 4.92 s\n",
      "Wall time: 4.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# грязный хак, чтобы не учить КНН на новом юзере каждый раз\n",
    "df_all['fake_customer_id'] = (pd.factorize(df_all.customer_id)[0] + 1) * 100\n",
    "\n",
    "points = df_all[['fake_customer_id', 'add_lat', 'add_lon']].drop_duplicates().values\n",
    "neigh = NearestNeighbors(2, radius=100000)\n",
    "\n",
    "# расстояние до уникальных точек\n",
    "# neigh.fit(np.unique(points, axis=1))\n",
    "neigh.fit(points) \n",
    "\n",
    "distances, indices = neigh.kneighbors(df_all[['fake_customer_id', 'add_lat', 'add_lon']].values)\n",
    "df_all['distance_to_nearest_point'] = distances[:, 1]\n",
    "del df_all['fake_customer_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19997/19997 [14:11<00:00, 23.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# фичи с кластерами из тинькова\n",
    "dfs = []\n",
    "customers = df_all.customer_id.unique()\n",
    "np_values = df_all[['customer_id', 'add_lat', 'add_lon']].values\n",
    "\n",
    "for i in tqdm(range(len(customers))):\n",
    "    customer = customers[i]\n",
    "    points = np_values[np_values[:, 0] == customer][:, 1:]\n",
    "    # оцениваем число кластеров\n",
    "#     avgs = []\n",
    "#     max_cluster = min(10,len(points))\n",
    "#     for i in range(2,max_cluster):\n",
    "#         kmeans = KMeans(n_clusters=i, random_state=2).fit(points)\n",
    "#         labels = kmeans.labels_\n",
    "#         silhouette_avg = silhouette_score(points, labels)\n",
    "#         avgs.append(silhouette_avg)\n",
    "        \n",
    "#     if max_cluster == 2:\n",
    "#         kmeans = KMeans(n_clusters=2, random_state=2).fit(points)\n",
    "#         labels = kmeans.labels_\n",
    "#         silhouette_avg = silhouette_score(points, labels)\n",
    "#         avgs.append(silhouette_avg)\n",
    "        \n",
    "#     n_cluster = avgs.index(max(avgs)) + 2 # так как индексы с 0 а кластеры с 2\n",
    "    # получаем лучший кластер\n",
    "    if np.unique(points).size == 2:\n",
    "        dfs.append(np.zeros((len(points), 4)))\n",
    "        continue\n",
    "    n_cluster = 2\n",
    "    kmeans = KMeans(n_clusters=n_cluster, random_state=2).fit(points)\n",
    "    #kmeans = AgglomerativeClustering(n_clusters=n_cluster,linkage='average').fit(points)\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    silhouette_avg = silhouette_score(points, labels)\n",
    "    # формируем датафрейм\n",
    "    sample_silhouette_values = silhouette_samples(points, labels)\n",
    "#     cluster_df = pd.DataFrame(data=np.vstack((labels, sample_silhouette_values)).T,columns=['label','score'])\n",
    "#     cluster_df.label = cluster_df.label.astype(np.int32)\n",
    "#     cluster_df['cluster_center_lat'] = cluster_df.apply(lambda row: centers[int(row['label'])][0], axis=1)\n",
    "#     cluster_df['cluster_center_lon'] = cluster_df.apply(lambda row: centers[int(row['label'])][1], axis=1)\n",
    "    arr_label_score = np.vstack((labels, sample_silhouette_values)).T\n",
    "    arr_label_score = np.hstack([arr_label_score, centers[labels]])\n",
    "    dfs.append(arr_label_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.DataFrame(np.vstack(dfs), columns=['cl_label','cl_score', 'cl_lat', 'cl_lon'])\n",
    "df_all.reset_index(inplace=True, drop=True)\n",
    "df_all = pd.concat([df_all, df_cluster], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('../data/df_all_1.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем признаки is_home, is_work\n",
    "TODO: удалить чуваков у которых несколько домов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = df_all['home_lat'] - df_all['add_lat']\n",
    "lon = df_all['home_lon'] - df_all['add_lon']\n",
    "\n",
    "df_all['is_home'] = (np.sqrt((lat ** 2) + (lon ** 2)) <= 0.02).astype(np.int8)\n",
    "df_all['has_home'] = (~df_all['home_lon'].isnull()).astype(np.int8)\n",
    "\n",
    "lat = df_all['work_lat'] - df_all['add_lat']\n",
    "lon = df_all['work_lon'] - df_all['add_lon']\n",
    "df_all['is_work'] = (np.sqrt((lat ** 2) + (lon ** 2)) <= 0.02).astype(np.int8)\n",
    "df_all['has_work'] = (~df_all['work_lon'].isnull()).astype(np.int8)\n",
    "\n",
    "df_all.drop(['work_lat','work_lon','home_lat','home_lon'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем категориальный признак для адреса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['address'] = df_all['add_lat'].apply(lambda x: \"%.02f\" % x) + ';' + df_all['add_lon'].apply(lambda x: \"%.02f\" % x)\n",
    "df_all['address'] = df_all['address'].factorize()[0].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем несколько абонентских фич"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество транзакций каждого клиента\n",
    "df_all = df_all.merge(df_all.groupby('customer_id')['amount'].count().reset_index(name='cid_trans_count'), how='left')\n",
    "df_all['cid_trans_count'] = df_all['cid_trans_count'].astype(np.int32)\n",
    "\n",
    "df_all = df_all.merge(df_all.groupby(['customer_id','address'])['amount'].count().reset_index(name='cid_add_trans_count'), \n",
    "                      how='left')\n",
    "df_all['cid_add_trans_count'] = df_all['cid_add_trans_count'].astype(np.int32)\n",
    "\n",
    "# какая часть транзакций клиента приходится на данный адрес\n",
    "# TODO: БОЛЬШЕ ТАКИХ ФИЧ\n",
    "df_all['ratio1'] = df_all['cid_add_trans_count'] / df_all['cid_trans_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мои фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# добавим признаки после групбая\n",
    "df_gb = df_all[['customer_id','amount', 'add_lat', 'add_lon']].groupby('customer_id')\n",
    "coord_stat_df = df_gb.agg(['mean', 'max', 'min'])\n",
    "coord_stat_df['transactions_per_user'] = df_gb.agg('size')\n",
    "coord_stat_df.columns = ['_'.join(col).strip() for col in coord_stat_df.columns.values]\n",
    "coord_stat_df.reset_index(inplace=True)\n",
    "df_all = pd.merge(df_all, coord_stat_df, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['add_lat', 'add_lon']\n",
    "types = ['min', 'max', 'mean']\n",
    "for c in cols:\n",
    "    for t in types:\n",
    "        df_all['{}_diff_{}'.format(c, t)] = np.abs(df_all[c] - df_all['{}_{}'.format(c, t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разности \n",
    "df_all['lat_diff_cluster_lat'] = np.abs(df_all['add_lat'] - df_all['cl_lat'])\n",
    "df_all['lon_diff_cluster_lon'] = np.abs(df_all['add_lon'] - df_all['cl_lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all, pd.get_dummies(df_all['mcc'], prefix='mcc')], axis=1)\n",
    "del df_all['mcc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.loc[:,~df_all.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ys = ['is_home', 'is_work']\n",
    "drop_cols = ['atm_address', 'customer_id', 'pos_address', 'terminal_id', 'transaction_date', \n",
    "             'is_home' ,'has_home', 'is_work', 'has_work', 'is_train']\n",
    "\n",
    "drop_cols += ['pred:is_home', 'pred:is_work']\n",
    "y_cols = ['is_home', 'is_work']\n",
    "usecols = df_all.drop(drop_cols, 1, errors='ignore').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 63,\n",
    "    'learning_rate': 0.01,\n",
    "    'metric' : 'binary_logloss',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'num_threads': 12,\n",
    "    'verbose': 0,\n",
    "}\n",
    "\n",
    "model = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds.\n",
      "[30]\tvalid_0's binary_logloss: 0.598695\n",
      "[60]\tvalid_0's binary_logloss: 0.541358\n",
      "[90]\tvalid_0's binary_logloss: 0.504476\n",
      "[120]\tvalid_0's binary_logloss: 0.479779\n",
      "[150]\tvalid_0's binary_logloss: 0.46284\n",
      "[180]\tvalid_0's binary_logloss: 0.450269\n",
      "[210]\tvalid_0's binary_logloss: 0.441381\n",
      "[240]\tvalid_0's binary_logloss: 0.434503\n",
      "[270]\tvalid_0's binary_logloss: 0.430182\n",
      "[300]\tvalid_0's binary_logloss: 0.427002\n",
      "[330]\tvalid_0's binary_logloss: 0.424357\n",
      "[360]\tvalid_0's binary_logloss: 0.422388\n",
      "[390]\tvalid_0's binary_logloss: 0.420946\n",
      "[420]\tvalid_0's binary_logloss: 0.420155\n",
      "[450]\tvalid_0's binary_logloss: 0.419309\n",
      "[480]\tvalid_0's binary_logloss: 0.418406\n",
      "[510]\tvalid_0's binary_logloss: 0.417984\n",
      "[540]\tvalid_0's binary_logloss: 0.417715\n",
      "[570]\tvalid_0's binary_logloss: 0.41724\n",
      "[600]\tvalid_0's binary_logloss: 0.416979\n",
      "[630]\tvalid_0's binary_logloss: 0.416843\n",
      "[660]\tvalid_0's binary_logloss: 0.416772\n",
      "[690]\tvalid_0's binary_logloss: 0.416764\n",
      "[720]\tvalid_0's binary_logloss: 0.416727\n",
      "[750]\tvalid_0's binary_logloss: 0.416771\n",
      "[780]\tvalid_0's binary_logloss: 0.416636\n",
      "[810]\tvalid_0's binary_logloss: 0.416781\n",
      "[840]\tvalid_0's binary_logloss: 0.416817\n",
      "[870]\tvalid_0's binary_logloss: 0.416953\n",
      "[900]\tvalid_0's binary_logloss: 0.416995\n",
      "[930]\tvalid_0's binary_logloss: 0.417117\n",
      "[960]\tvalid_0's binary_logloss: 0.417216\n",
      "[990]\tvalid_0's binary_logloss: 0.417321\n",
      "[1020]\tvalid_0's binary_logloss: 0.41745\n",
      "[1050]\tvalid_0's binary_logloss: 0.417587\n",
      "[1080]\tvalid_0's binary_logloss: 0.417727\n",
      "Early stopping, best iteration is:\n",
      "[783]\tvalid_0's binary_logloss: 0.416622\n"
     ]
    }
   ],
   "source": [
    "y_col = 'is_home'\n",
    "\n",
    "cust_train = df_all[df_all['is_train']==1].groupby('customer_id')[y_col.replace('is_','has_')].max()\n",
    "cust_train = cust_train[cust_train > 0].index\n",
    "\n",
    "cust_train, cust_valid = train_test_split(cust_train, test_size=0.2, shuffle=True, random_state=111)\n",
    "\n",
    "df_train = pd.DataFrame(cust_train, columns=['customer_id']).merge(df_all, how='left')\n",
    "df_valid = pd.DataFrame(cust_valid, columns=['customer_id']).merge(df_all, how='left')\n",
    "\n",
    "lgb_train = lgb.Dataset(df_train[usecols], df_train[y_col])\n",
    "lgb_valid = lgb.Dataset(df_valid[usecols], df_valid[y_col])\n",
    "\n",
    "gbm_h = lgb.train(params,\n",
    "                lgb_train,\n",
    "                valid_sets=[lgb_valid],\n",
    "                num_boost_round=2000,\n",
    "                verbose_eval=30,\n",
    "                early_stopping_rounds=300)\n",
    "\n",
    "model[y_col] = gbm_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds.\n",
      "[30]\tvalid_0's binary_logloss: 0.577558\n",
      "[60]\tvalid_0's binary_logloss: 0.506715\n",
      "[90]\tvalid_0's binary_logloss: 0.460817\n",
      "[120]\tvalid_0's binary_logloss: 0.42959\n",
      "[150]\tvalid_0's binary_logloss: 0.408391\n",
      "[180]\tvalid_0's binary_logloss: 0.393525\n",
      "[210]\tvalid_0's binary_logloss: 0.382874\n",
      "[240]\tvalid_0's binary_logloss: 0.375798\n",
      "[270]\tvalid_0's binary_logloss: 0.370509\n",
      "[300]\tvalid_0's binary_logloss: 0.366902\n",
      "[330]\tvalid_0's binary_logloss: 0.363107\n",
      "[360]\tvalid_0's binary_logloss: 0.360295\n",
      "[390]\tvalid_0's binary_logloss: 0.357828\n",
      "[420]\tvalid_0's binary_logloss: 0.355896\n",
      "[450]\tvalid_0's binary_logloss: 0.354187\n",
      "[480]\tvalid_0's binary_logloss: 0.353145\n",
      "[510]\tvalid_0's binary_logloss: 0.352088\n",
      "[540]\tvalid_0's binary_logloss: 0.35114\n",
      "[570]\tvalid_0's binary_logloss: 0.350587\n",
      "[600]\tvalid_0's binary_logloss: 0.350096\n",
      "[630]\tvalid_0's binary_logloss: 0.34949\n",
      "[660]\tvalid_0's binary_logloss: 0.34942\n",
      "[690]\tvalid_0's binary_logloss: 0.349329\n",
      "[720]\tvalid_0's binary_logloss: 0.349252\n",
      "[750]\tvalid_0's binary_logloss: 0.349075\n",
      "[780]\tvalid_0's binary_logloss: 0.349019\n",
      "[810]\tvalid_0's binary_logloss: 0.349235\n",
      "[840]\tvalid_0's binary_logloss: 0.34954\n",
      "[870]\tvalid_0's binary_logloss: 0.349509\n",
      "[900]\tvalid_0's binary_logloss: 0.349353\n",
      "[930]\tvalid_0's binary_logloss: 0.349637\n",
      "[960]\tvalid_0's binary_logloss: 0.349515\n",
      "[990]\tvalid_0's binary_logloss: 0.349572\n",
      "[1020]\tvalid_0's binary_logloss: 0.349632\n",
      "[1050]\tvalid_0's binary_logloss: 0.349565\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.348902\n"
     ]
    }
   ],
   "source": [
    "y_col = 'is_work'\n",
    "\n",
    "cust_train = df_all[df_all['is_train']==1].groupby('customer_id')[y_col.replace('is_','has_')].max()\n",
    "cust_train = cust_train[cust_train > 0].index\n",
    "\n",
    "cust_train, cust_valid = train_test_split(cust_train, test_size=0.2, shuffle=True, random_state=111)\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(cust_train, columns=['customer_id']).merge(df_all, how='left')\n",
    "df_valid = pd.DataFrame(cust_valid, columns=['customer_id']).merge(df_all, how='left')\n",
    "\n",
    "lgb_train = lgb.Dataset(df_train[usecols], df_train[y_col])\n",
    "lgb_valid = lgb.Dataset(df_valid[usecols], df_valid[y_col])\n",
    "\n",
    "gbm_w = lgb.train(params,\n",
    "                lgb_train,\n",
    "                valid_sets=[lgb_valid],\n",
    "                num_boost_round=2000,\n",
    "                verbose_eval=30,\n",
    "                early_stopping_rounds=300)\n",
    "\n",
    "model[y_col] = gbm_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(gbm_w, max_num_features=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _best(x):\n",
    "    ret = None\n",
    "    for col in ys:\n",
    "        pred = ('pred:%s' % col)\n",
    "        if pred in x:\n",
    "            i = (x[pred].idxmax())\n",
    "            cols = [pred, 'add_lat', 'add_lon']\n",
    "            if col in x:\n",
    "                cols.append(col)\n",
    "            tmp = x.loc[i,cols]\n",
    "            tmp.rename({\n",
    "                'add_lat':'%s:add_lat' % col,\n",
    "                'add_lon':'%s:add_lon' % col,\n",
    "            }, inplace = True)\n",
    "            if ret is None:\n",
    "                ret = tmp\n",
    "            else:\n",
    "                ret = pd.concat([ret, tmp])\n",
    "    return ret\n",
    "\n",
    "def predict_proba(dt, ys=['is_home', 'is_work']):\n",
    "    for col in ys:\n",
    "        pred = ('pred:%s' % col)\n",
    "        dt[pred] = model[col].predict(dt[usecols])\n",
    "    return dt.groupby('customer_id').apply(_best).reset_index()\n",
    "\n",
    "def score(dt, ys=['is_home', 'is_work']):\n",
    "    dt_ret = predict_proba(dt, ys)\n",
    "    mean = 0.0\n",
    "    for col in ys:\n",
    "        col_mean = dt_ret[col].mean()\n",
    "        mean += col_mean\n",
    "    if len(ys) == 2:\n",
    "        mean = mean / len(ys)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5087251575375666\n",
      "Test accuracy: 0.5096899224806202\n",
      "Train accuracy: 0.37930198739699467\n",
      "Test accuracy: 0.3430232558139535\n"
     ]
    }
   ],
   "source": [
    "print (\"Train accuracy:\", score(df_train, ys=['is_home']))\n",
    "print (\"Test accuracy:\", score(df_valid, ys=['is_home']))\n",
    "\n",
    "print (\"Train accuracy:\", score(df_train, ys=['is_work']))\n",
    "print (\"Test accuracy:\", score(df_valid, ys=['is_work']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_test = df_all[df_all['is_train'] == 0]['customer_id'].unique()\n",
    "df_test = pd.DataFrame(cust_test, columns = ['customer_id']).merge(df_all, how = 'left')\n",
    "df_test = predict_proba(df_test)\n",
    "df_test.rename(columns = {\n",
    "        'customer_id':'_ID_',\n",
    "        'is_home:add_lat': '_HOME_LAT_',\n",
    "        'is_home:add_lon': '_HOME_LON_',\n",
    "        'is_work:add_lat': '_WORK_LAT_',\n",
    "        'is_work:add_lon': '_WORK_LON_'}, inplace = True)\n",
    "df_test = df_test[['_ID_', '_WORK_LAT_', '_WORK_LON_', '_HOME_LAT_', '_HOME_LON_']]\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем submission-файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем пропуски\n",
    "df_ = pd.read_csv('../data/test_set.csv', dtype=dtypes, usecols=['customer_id'])\n",
    "submission = pd.DataFrame(df_['customer_id'].unique(), columns=['_ID_'])\n",
    "\n",
    "submission = submission.merge(df_test, how='left').fillna(0)\n",
    "# Пишем файл submission\n",
    "submission.to_csv('../submissions/base_3_50_34.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
